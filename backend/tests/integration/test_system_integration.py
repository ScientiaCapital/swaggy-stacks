"""
Comprehensive System Integration Tests
Tests all major system improvements while protecting intellectual property
"""

import asyncio
import pytest
import psutil
import time
import os
import sys
from typing import Dict, List, Any, Optional
from unittest.mock import Mock, AsyncMock, patch
import pandas as pd
import numpy as np
from datetime import datetime, timedelta

# Add backend to path for imports
sys.path.append('/Users/tmkipper/repos/swaggy-stacks/backend')

from app.ai.ollama_client import OllamaClient
from app.ai.model_serving_strategy import MemoryEfficientServingStrategy, DeploymentMode, ModelSize
from app.ai.mlx_config import MLXConfigManager
from app.analysis.pattern_validation_framework import PatternValidationFramework
from app.analysis.integrated_alpha_detector import IntegratedAlphaDetector


class PerformanceMonitor:
    """Monitor system performance during tests"""
    
    def __init__(self):
        self.process = psutil.Process()
        self.start_time = None
        self.start_memory = None
        self.measurements = []
    
    def start_monitoring(self):
        """Start performance monitoring"""
        self.start_time = time.time()
        self.start_memory = self.process.memory_info().rss / 1024 / 1024  # MB
        self.measurements = []
    
    def record_measurement(self, operation: str):
        """Record a performance measurement"""
        current_memory = self.process.memory_info().rss / 1024 / 1024  # MB
        current_time = time.time()
        
        self.measurements.append({
            'operation': operation,
            'timestamp': current_time,
            'memory_mb': current_memory,
            'elapsed_time': current_time - self.start_time if self.start_time else 0
        })
    
    def get_summary(self) -> Dict[str, Any]:
        """Get performance summary"""
        if not self.measurements:
            return {'error': 'No measurements recorded'}
        
        memory_values = [m['memory_mb'] for m in self.measurements]
        
        return {
            'total_duration': time.time() - self.start_time if self.start_time else 0,
            'peak_memory_mb': max(memory_values),
            'average_memory_mb': sum(memory_values) / len(memory_values),
            'memory_increase_mb': max(memory_values) - self.start_memory if self.start_memory else 0,
            'measurement_count': len(self.measurements),
            'final_memory_mb': memory_values[-1] if memory_values else 0
        }


@pytest.fixture
def performance_monitor():
    """Performance monitoring fixture"""
    return PerformanceMonitor()


@pytest.fixture
def sample_market_data():
    """Safe mock market data for testing"""
    return {
        'symbol': 'AAPL',
        'current_price': 150.0,
        'volume': 50000000,
        'high_52w': 180.0,
        'low_52w': 120.0,
        'volatility': 0.25,
        'timestamp': datetime.utcnow(),
        'historical_data': pd.DataFrame({
            'close': np.random.normal(150, 5, 100),
            'high': np.random.normal(152, 5, 100),
            'low': np.random.normal(148, 5, 100),
            'volume': np.random.normal(50000000, 10000000, 100),
            'timestamp': pd.date_range('2024-01-01', periods=100, freq='H')
        })
    }


@pytest.fixture
def sample_technical_indicators():
    """Safe mock technical indicators"""
    return {
        'rsi': 65.0,
        'ma20': 148.0,
        'ma50': 145.0,
        'ma200': 140.0,
        'macd': 2.5,
        'macd_signal': 2.2,
        'macd_histogram': 0.3,
        'atr': 3.2,
        'bollinger_upper': 155.0,
        'bollinger_lower': 145.0
    }


class TestMLXConfiguration:
    """Test MLX framework integration and configuration"""
    
    def test_mlx_config_initialization(self, performance_monitor):
        """Test MLX configuration manager initialization"""
        performance_monitor.start_monitoring()
        
        try:
            config_manager = MLXConfigManager()
            performance_monitor.record_measurement("MLX config init")
            
            assert config_manager is not None
            assert hasattr(config_manager, 'is_apple_silicon')
            assert hasattr(config_manager, 'device_config')
            
            # Test system detection
            system_info = config_manager.detect_system_capabilities()
            assert 'platform' in system_info
            assert 'memory_gb' in system_info
            assert 'mlx_available' in system_info
            
        except ImportError:
            # MLX not available - test fallback behavior
            pytest.skip("MLX not available - testing fallback")
        
        performance_monitor.record_measurement("MLX config complete")
        summary = performance_monitor.get_summary()
        
        # Verify reasonable memory usage for config initialization
        assert summary['memory_increase_mb'] < 100  # Should be lightweight
    
    def test_memory_budget_management(self):
        """Test memory budget validation for M1 Mac constraints"""
        # Test with 8GB constraint
        strategy = MemoryEfficientServingStrategy(
            available_memory_gb=8.0, 
            deployment_mode=DeploymentMode.HYBRID
        )
        
        assert strategy.available_memory_gb == 8.0
        assert strategy.deployment_mode == DeploymentMode.HYBRID
        
        # Test model selection under memory constraints
        selected_models = strategy.select_optimal_models()
        
        # Verify memory constraints are respected
        total_local_memory = sum(
            model.memory_requirement_gb 
            for model in selected_models 
            if model.memory_requirement_gb <= 8.0
        )
        
        # Should not exceed 80% of available memory (safety margin)
        assert total_local_memory <= 6.4  # 80% of 8GB
    
    def test_deployment_mode_configuration(self):
        """Test different deployment mode configurations"""
        modes_to_test = [
            DeploymentMode.NATIVE_MLX,
            DeploymentMode.HYBRID,
            DeploymentMode.CONTAINER_CPU,
            DeploymentMode.API_ONLY
        ]
        
        for mode in modes_to_test:
            strategy = MemoryEfficientServingStrategy(
                available_memory_gb=8.0,
                deployment_mode=mode
            )
            
            config = strategy.generate_deployment_config()
            
            assert config['deployment_mode'] == mode.value
            assert 'memory_allocation' in config
            assert 'local_models' in config
            assert 'api_models' in config
            
            # API_ONLY mode should have all models as API models
            if mode == DeploymentMode.API_ONLY:
                assert len(config['local_models']) == 0
                assert len(config['api_models']) > 0


class TestChineseLLMModels:
    """Test Chinese LLM model integration and memory management"""
    
    @pytest.mark.asyncio
    async def test_ollama_client_initialization(self, performance_monitor):
        """Test Ollama client initialization with new Chinese models"""
        performance_monitor.start_monitoring()
        
        client = OllamaClient()
        performance_monitor.record_measurement("Ollama client init")
        
        # Test that new Chinese models are available
        chinese_models = ['yi_technical', 'glm_risk', 'qwen_quant', 'deepseek_lite']
        
        for model_key in chinese_models:
            model_info = client.get_model_info(model_key)
            assert model_info is not None
            assert model_info.memory_usage_mb > 0
            assert model_info.memory_usage_mb <= 8000  # Within 8GB limit
            
            performance_monitor.record_measurement(f"Model {model_key} info")
        
        summary = performance_monitor.get_summary()
        assert summary['memory_increase_mb'] < 50  # Initialization should be lightweight
    
    @pytest.mark.asyncio
    async def test_memory_budget_status(self):
        """Test memory budget monitoring for Chinese LLM models"""
        client = OllamaClient()
        
        # Test memory budget status for M1 Mac (8GB)
        budget_status = client.get_memory_budget_status(max_memory_mb=7500)  # 7.5GB usable
        
        assert 'total_usage_mb' in budget_status
        assert 'max_budget_mb' in budget_status
        assert 'available_mb' in budget_status
        assert 'usage_percentage' in budget_status
        assert 'memory_efficient' in budget_status
        
        # Should be memory efficient initially
        assert budget_status['memory_efficient']
        assert budget_status['usage_percentage'] <= 100
    
    @pytest.mark.asyncio
    async def test_model_memory_optimization(self):
        """Test memory optimization strategies"""
        client = OllamaClient()
        
        # Simulate high memory usage by adding context
        for i in range(10):
            client.context_history[f'test_context_{i}'] = {'test': 'data'}
        
        # Test memory optimization
        optimization_result = client.optimize_memory_usage()
        
        assert 'actions_taken' in optimization_result
        assert 'estimated_memory_freed_mb' in optimization_result
        assert 'optimization_successful' in optimization_result
        
        # Should have taken some optimization actions
        if len(client.context_history) > 3:
            assert optimization_result['optimization_successful']
    
    @pytest.mark.asyncio
    async def test_chinese_model_selection(self):
        """Test Chinese model selection for specific tasks"""
        client = OllamaClient()
        
        # Test task-based model recommendations
        task_mappings = {
            'technical_analysis': 'analyst',
            'risk_assessment': 'risk',
            'quantitative_analysis': 'strategist',
            'conversational': 'chat'
        }
        
        for task_type, expected_base in task_mappings.items():
            recommended_model = client.get_recommended_model_for_task(task_type)
            assert recommended_model is not None
            # Should recommend an appropriate model (could be enhanced Chinese version)
            assert recommended_model in client.MODELS


class TestPatternDetection:
    """Test pattern library expansion and validation"""
    
    def test_pattern_validation_framework(self, sample_market_data, performance_monitor):
        """Test pattern validation framework initialization"""
        performance_monitor.start_monitoring()
        
        try:
            validator = PatternValidationFramework()
            performance_monitor.record_measurement("Pattern validator init")
            
            assert validator is not None
            
            # Test validation with mock data
            validation_result = validator.validate_pattern_performance(
                pattern_name="test_pattern",
                market_data=sample_market_data,
                expected_accuracy=0.7
            )
            
            assert 'pattern_name' in validation_result
            assert 'performance_score' in validation_result
            assert 'validation_status' in validation_result
            
            performance_monitor.record_measurement("Pattern validation")
            
        except ImportError as e:
            pytest.skip(f"Pattern validation framework not available: {e}")
        
        summary = performance_monitor.get_summary()
        assert summary['memory_increase_mb'] < 200  # Should be reasonable
    
    def test_integrated_alpha_detector(self, sample_market_data, sample_technical_indicators, performance_monitor):
        """Test integrated alpha detector with multiple methodologies"""
        performance_monitor.start_monitoring()
        
        try:
            detector = IntegratedAlphaDetector()
            performance_monitor.record_measurement("Alpha detector init")
            
            # Test parallel analysis integration
            analysis_result = detector.analyze_parallel(
                symbol="AAPL",
                market_data=sample_market_data,
                technical_indicators=sample_technical_indicators
            )
            
            assert 'markov_analysis' in analysis_result
            assert 'elliott_wave_analysis' in analysis_result
            assert 'fibonacci_analysis' in analysis_result
            assert 'golden_zone_analysis' in analysis_result
            assert 'wyckoff_analysis' in analysis_result
            assert 'alpha_patterns' in analysis_result
            assert 'integrated_signal' in analysis_result
            
            # Verify split-second decision capability
            start_time = time.time()
            quick_decision = detector.get_trading_decision(analysis_result)
            decision_time = time.time() - start_time
            
            assert decision_time < 1.0  # Should be sub-second
            assert 'action' in quick_decision
            assert 'confidence' in quick_decision
            assert 'reasoning' in quick_decision
            
            performance_monitor.record_measurement("Alpha detection complete")
            
        except ImportError as e:
            pytest.skip(f"Integrated alpha detector not available: {e}")
        
        summary = performance_monitor.get_summary()
        assert summary['memory_increase_mb'] < 300  # Pattern detection should be memory-efficient
    
    def test_pattern_library_expansion(self):
        """Test that pattern library has been expanded"""
        # This tests the expansion without exposing specific patterns
        try:
            from backend.private_ai_modules.superbpe_trading_tokenizer import SuperBPETradingTokenizer
            
            tokenizer = SuperBPETradingTokenizer()
            pattern_mappings = tokenizer.get_pattern_mappings()
            
            # Should have the new categories we added
            expected_categories = [
                'momentum_alpha',
                'volatility_alpha', 
                'cross_asset_alpha',
                'microstructure_alpha',
                'options_flow_alpha',
                'regime_alpha'
            ]
            
            for category in expected_categories:
                assert category in pattern_mappings
                # Each category should have multiple patterns
                assert len(pattern_mappings[category]) >= 5
            
        except ImportError:
            pytest.skip("SuperBPE tokenizer not available for testing")


class TestTradingIntegration:
    """Test live trading system integration"""
    
    @pytest.mark.asyncio
    async def test_trading_decision_speed(self, sample_market_data, sample_technical_indicators, performance_monitor):
        """Test that trading decisions are made in sub-second time"""
        performance_monitor.start_monitoring()
        
        try:
            detector = IntegratedAlphaDetector()
            
            # Test multiple rapid decisions to simulate real trading
            decision_times = []
            
            for i in range(10):
                start_time = time.time()
                
                # Simulate slightly different market conditions
                test_data = sample_market_data.copy()
                test_data['current_price'] = 150.0 + i * 0.5
                
                analysis_result = detector.analyze_parallel(
                    symbol="AAPL",
                    market_data=test_data,
                    technical_indicators=sample_technical_indicators
                )
                
                decision = detector.get_trading_decision(analysis_result)
                decision_time = time.time() - start_time
                decision_times.append(decision_time)
                
                performance_monitor.record_measurement(f"Decision {i+1}")
            
            # All decisions should be sub-second
            average_decision_time = sum(decision_times) / len(decision_times)
            max_decision_time = max(decision_times)
            
            assert average_decision_time < 0.5  # Average should be very fast
            assert max_decision_time < 1.0      # Even slowest should be sub-second
            
        except ImportError:
            pytest.skip("Trading integration components not available")
        
        summary = performance_monitor.get_summary()
        print(f"Trading decision performance: avg={average_decision_time:.3f}s, max={max_decision_time:.3f}s")
    
    @pytest.mark.asyncio
    async def test_risk_management_integration(self, sample_market_data):
        """Test enhanced risk management integration"""
        # Test risk assessment with mock data
        risk_data = {
            'portfolio_value': 100000.0,
            'current_positions': [
                {'symbol': 'AAPL', 'value': 10000, 'risk_percent': 2.0},
                {'symbol': 'GOOGL', 'value': 15000, 'risk_percent': 3.0}
            ],
            'volatility_data': {
                'vix': 20.5,
                'portfolio_beta': 1.2,
                'correlation_risk': 0.3
            }
        }
        
        # Mock risk assessment (without exposing real risk algorithms)
        mock_risk_result = {
            'overall_risk_level': 'medium',
            'portfolio_heat': 5.0,
            'recommended_position_size': 0.02,
            'risk_factors': ['volatility', 'correlation'],
            'max_drawdown_estimate': 0.15
        }
        
        assert mock_risk_result['overall_risk_level'] in ['low', 'medium', 'high']
        assert 0 <= mock_risk_result['portfolio_heat'] <= 100
        assert 0 <= mock_risk_result['recommended_position_size'] <= 1.0


class TestMemoryManagement:
    """Test memory management and optimization"""
    
    def test_memory_efficient_model_serving(self):
        """Test memory-efficient model serving strategy"""
        # Test with constrained memory (8GB M1 Mac)
        strategy = MemoryEfficientServingStrategy(available_memory_gb=8.0)
        
        selected_models = strategy.select_optimal_models()
        
        # Calculate total memory usage of selected local models
        local_memory_usage = sum(
            model.memory_requirement_gb 
            for model in selected_models 
            if model.memory_requirement_gb <= 8.0
        )
        
        # Should respect memory constraints
        assert local_memory_usage <= 6.4  # 80% of 8GB
        
        # Should have some models selected
        assert len(selected_models) > 0
        
        # Should prioritize high-priority models
        priority_1_models = [m for m in selected_models if m.priority == 1]
        assert len(priority_1_models) > 0
    
    def test_memory_monitoring_during_operations(self, performance_monitor):
        """Test memory monitoring during various operations"""
        performance_monitor.start_monitoring()
        
        # Simulate various memory-intensive operations
        operations = [
            ("Create OllamaClient", lambda: OllamaClient()),
            ("Create MemoryStrategy", lambda: MemoryEfficientServingStrategy()),
            ("Generate mock data", lambda: pd.DataFrame(np.random.random((1000, 10))))
        ]
        
        objects = []
        for operation_name, operation in operations:
            obj = operation()
            objects.append(obj)
            performance_monitor.record_measurement(operation_name)
        
        summary = performance_monitor.get_summary()
        
        # Memory usage should be reasonable
        assert summary['peak_memory_mb'] < 1000  # Less than 1GB peak
        assert summary['memory_increase_mb'] < 500  # Reasonable increase
        
        # Clean up objects to test memory release
        del objects
        performance_monitor.record_measurement("Cleanup")
    
    def test_context_cache_optimization(self):
        """Test context cache optimization in Ollama client"""
        client = OllamaClient()
        
        # Fill up context cache
        for i in range(10):
            client.context_history[f'context_{i}'] = {'data': f'test_data_{i}'}
        
        initial_cache_size = len(client.context_history)
        
        # Trigger optimization
        result = client.optimize_memory_usage()
        
        final_cache_size = len(client.context_history)
        
        # Should have optimized if cache was too large
        if initial_cache_size > 5:
            assert final_cache_size < initial_cache_size
            assert result['optimization_successful']
            assert len(result['actions_taken']) > 0


class TestSystemPerformance:
    """Test overall system performance and benchmarks"""
    
    @pytest.mark.asyncio
    async def test_concurrent_model_operations(self, performance_monitor):
        """Test system performance with concurrent operations"""
        performance_monitor.start_monitoring()
        
        client = OllamaClient()
        
        # Test concurrent memory budget checks (simulates concurrent requests)
        async def check_memory_budget():
            return client.get_memory_budget_status()
        
        # Run multiple concurrent operations
        tasks = [check_memory_budget() for _ in range(5)]
        start_time = time.time()
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        concurrent_time = time.time() - start_time
        performance_monitor.record_measurement("Concurrent operations")
        
        # All operations should succeed
        successful_results = [r for r in results if not isinstance(r, Exception)]
        assert len(successful_results) == 5
        
        # Should handle concurrent operations efficiently
        assert concurrent_time < 2.0  # Should complete quickly
        
        summary = performance_monitor.get_summary()
        print(f"Concurrent operations completed in {concurrent_time:.3f}s")
    
    def test_memory_usage_benchmarks(self, performance_monitor):
        """Benchmark memory usage for M1 Mac optimization"""
        performance_monitor.start_monitoring()
        
        # Baseline memory
        baseline_memory = performance_monitor.process.memory_info().rss / 1024 / 1024
        performance_monitor.record_measurement("Baseline")
        
        # Create components
        client = OllamaClient()
        performance_monitor.record_measurement("OllamaClient created")
        
        strategy = MemoryEfficientServingStrategy()
        performance_monitor.record_measurement("MemoryStrategy created")
        
        try:
            validator = PatternValidationFramework()
            performance_monitor.record_measurement("PatternValidator created")
        except ImportError:
            pass
        
        summary = performance_monitor.get_summary()
        
        # Memory benchmarks for M1 Mac (8GB total)
        # System should use less than 25% of available RAM for AI components
        max_acceptable_memory = 2000  # 2GB
        
        assert summary['peak_memory_mb'] < max_acceptable_memory
        assert summary['memory_increase_mb'] < 1000  # Less than 1GB increase
        
        print(f"Memory benchmark: peak={summary['peak_memory_mb']:.1f}MB, increase={summary['memory_increase_mb']:.1f}MB")
    
    def test_system_resource_utilization(self):
        """Test overall system resource utilization"""
        # Monitor CPU and memory during operations
        process = psutil.Process()
        
        # Get initial stats
        initial_cpu_percent = process.cpu_percent()
        initial_memory_mb = process.memory_info().rss / 1024 / 1024
        
        # Perform some operations
        client = OllamaClient()
        budget_status = client.get_memory_budget_status()
        available_models = client.get_available_models()
        
        # Get final stats
        final_cpu_percent = process.cpu_percent()
        final_memory_mb = process.memory_info().rss / 1024 / 1024
        
        # Resource utilization should be reasonable
        cpu_increase = final_cpu_percent - initial_cpu_percent
        memory_increase = final_memory_mb - initial_memory_mb
        
        # Should not consume excessive resources
        assert memory_increase < 500  # Less than 500MB increase
        
        results = {
            'initial_memory_mb': initial_memory_mb,
            'final_memory_mb': final_memory_mb,
            'memory_increase_mb': memory_increase,
            'cpu_increase_percent': cpu_increase,
            'models_available': len(available_models),
            'memory_efficient': budget_status.get('memory_efficient', False)
        }
        
        print(f"Resource utilization: {results}")
        return results


# Integration test runner
@pytest.mark.asyncio
async def test_full_system_integration(performance_monitor):
    """Comprehensive end-to-end system integration test"""
    performance_monitor.start_monitoring()
    
    print("üöÄ Starting comprehensive system integration test...")
    
    # Test components in sequence
    test_results = {}
    
    try:
        # 1. MLX Configuration
        print("üì± Testing MLX configuration...")
        config_manager = MLXConfigManager()
        system_info = config_manager.detect_system_capabilities()
        test_results['mlx_config'] = system_info
        performance_monitor.record_measurement("MLX config test")
        
        # 2. Chinese LLM Models
        print("üá®üá≥ Testing Chinese LLM integration...")
        client = OllamaClient()
        chinese_models = ['yi_technical', 'glm_risk', 'qwen_quant', 'deepseek_lite']
        available_chinese_models = {
            model: client.get_model_info(model) is not None 
            for model in chinese_models
        }
        test_results['chinese_models'] = available_chinese_models
        performance_monitor.record_measurement("Chinese LLM test")
        
        # 3. Memory Management
        print("üß† Testing memory management...")
        budget_status = client.get_memory_budget_status()
        optimization_result = client.optimize_memory_usage()
        test_results['memory_management'] = {
            'budget_status': budget_status,
            'optimization_available': optimization_result['optimization_successful']
        }
        performance_monitor.record_measurement("Memory management test")
        
        # 4. Pattern Detection (if available)
        print("üìä Testing pattern detection...")
        try:
            detector = IntegratedAlphaDetector()
            sample_data = {
                'symbol': 'AAPL',
                'current_price': 150.0,
                'historical_data': pd.DataFrame({
                    'close': np.random.normal(150, 5, 50),
                    'volume': np.random.normal(1000000, 100000, 50)
                })
            }
            
            analysis_start = time.time()
            analysis_result = detector.analyze_parallel("AAPL", sample_data, {})
            analysis_time = time.time() - analysis_start
            
            test_results['pattern_detection'] = {
                'analysis_time_seconds': analysis_time,
                'analysis_completed': 'integrated_signal' in analysis_result,
                'sub_second_performance': analysis_time < 1.0
            }
            
        except ImportError:
            test_results['pattern_detection'] = {'status': 'not_available'}
        
        performance_monitor.record_measurement("Pattern detection test")
        
        # 5. Memory-Efficient Serving
        print("‚ö° Testing memory-efficient serving...")
        strategy = MemoryEfficientServingStrategy(available_memory_gb=8.0)
        selected_models = strategy.select_optimal_models()
        deployment_config = strategy.generate_deployment_config()
        
        test_results['memory_efficient_serving'] = {
            'models_selected': len(selected_models),
            'deployment_mode': deployment_config['deployment_mode'],
            'memory_optimized': len(deployment_config.get('local_models', [])) > 0
        }
        performance_monitor.record_measurement("Memory serving test")
        
    except Exception as e:
        test_results['error'] = str(e)
        print(f"‚ùå Error during integration test: {e}")
    
    performance_monitor.record_measurement("Full integration complete")
    summary = performance_monitor.get_summary()
    
    # Final results
    integration_results = {
        'test_results': test_results,
        'performance_summary': summary,
        'overall_success': 'error' not in test_results,
        'memory_efficient': summary['memory_increase_mb'] < 1000,
        'performance_acceptable': summary['total_duration'] < 30.0
    }
    
    print(f"‚úÖ Integration test completed in {summary['total_duration']:.2f}s")
    print(f"üìä Memory usage: {summary['memory_increase_mb']:.1f}MB increase")
    print(f"üéØ Tests passed: {integration_results['overall_success']}")
    
    return integration_results


if __name__ == "__main__":
    # Run the comprehensive integration test
    async def run_integration_test():
        monitor = PerformanceMonitor()
        results = await test_full_system_integration(monitor)
        
        print("\n" + "="*50)
        print("INTEGRATION TEST RESULTS")
        print("="*50)
        
        for key, value in results.items():
            print(f"{key}: {value}")
        
        return results
    
    # Run the test
    asyncio.run(run_integration_test())